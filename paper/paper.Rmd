---
title: "Learning from Data Through Models"
date: '`r format(Sys.time(), "%B %d, %Y")`'
author: "Alberto Bisin, Guillaume Frechette and Jimena Galindo"
abstract: "TBW"
#classoption: pagebackref # passes pagebackref=true to hyperref before it is loaded, allowing backreferencing
output: 
    pdf_document:
        citation_package: natbib
        fig_width: 7
        fig_height: 6
        fig_caption: true
        number_sections: true
        template: NULL
        keep_tex: true
        extra_dependencies:
            footmisc: ["bottom"] # footnote management
            setspace: ["doublespacing"] # spacing of the paper
            caption: ["normal"]
            dsfont: null # indicator function 1
            booktabs: null
            makecell: null
            hyperref: null
        includes:
          in_header: extra_header.tex

bibliography: bibliography.bib
fontsize: 12pt
geometry: margin=2.5cm
---

```{r dependencies, include=FALSE}
source("../parameters_and_packages.R")
```

```{r setup, include=FALSE}
# Format the numbers in the inline code output
inline_hook <- function(x){
  if(is.numeric(x)) {

    # integers get , to separate thousands
    if(abs(x - round(x)) < .Machine$double.eps){
      formatted <- format(x, big.mark = ",", scientific = FALSE)
    } else {
    # consistent rounding of non-integers
      formatted <- format(x, digits = 2, nsmall = 0, scientific = FALSE)
    }

  } else{

    # render text in bold (commented out)
    if(is.character(x)){
      # formatted <- paste0("**", x, "**")
      formatted <- x
    } else{
      formatted <- x
    }

  }

  formatted
}

knitr::knit_hooks$set(inline = inline_hook)
```

# Introduction


# Literature Review



# Framework and Predictions

A finite number of observable random variables, $X =(x_1, x_2, ..., x_N)$, determine a binary state, $y$. 
The observable variables are independently distributed and each follows a Normal distribution with corresponding mean 
$\mu_i$ and standard deviation $\sigma_i$. The state is \emph{Red} (**R**) or \emph{Blue} (**B**) and is determined in 
the following way:

\begin{equation*}
y = \begin{cases}
\text{\textbf{B}} &\text{if $a_1x_1+...+a_Nx_N + K\geq0$}\\
\text{\textbf{R}} &\text{otherwise}
\end{cases}
\end{equation*}

An observable variable $x_i$ is *relevant* if the associated coefficient, $a_i$, is non-zero. 
Likewise, variable $i$ is *irrelevant* if $a_i = 0$. We call the set of relevant variables $\mathcal{R}$ and the set of 
irrelevant variables $\mathcal{I}$.

In each period, an agent observes the realization of all variables and predicts the state. Their flow payoff in 
period $t$ is 1 if they predict the state correctly, and it is zero otherwise. Denote the prediction by $\hat{y}$. 

In order to make a prediction, the agent uses a map $M_t\to \{\textbf{B}, \textbf{R}\}$  where 
$M_t \subseteq \{x_1, x_2, ..., x_N\}$ is the set of variables that they choose to consider in period $t$. 
Our main object of study is the set of variables that the agent decides to consider in a given period. We
abstract away from the procedure by which the maping and the set are updated. 
We refer to the set of variables that are taken into consideration, $M_t$, as the agent's 
*mental model*, and assume that the mapping is linear.  

A mental model is *simple* if it does not include all the relevant variables, 
*i.e.* if $M\subset \mathcal{R}$. And it is *complex* if it includes all relevant variables, ($\mathcal{R}\subseteq M$). 
Mental models can be ranked in terms of their simplicity by the number of relevant variables that they consider: a model that 
considers only $x_1$ is simpler than one that considers $x_2$ and $x_3$. 

Agents that use simple models ignore some of the relevant information that is available to them. While agents who use comples models
use all of the relevant information.
This means that when agents with complex mental models are restricted in terms of how many variables they can consider, their ability
to predict the state will be impaired. On the contrary, restricting the number of variables that an agent with a simple model can 
use, does not necessarily affect their ability to predict the state. In particular, if they are considering fewer variables than 
what the restriciton allows, their predictions should be as accurate as without the restriciton. These two effects are captured in 
predictions 1 and 1B respectively.

**Prediction 1:** If subjects use complex models, then allowing them to use an additional variable will always 
increase the frequency with which they make correct predictions.

**Prediction 1B:** If subjects use a simple mental model that considers $K$ variables, allowing them to use $K+1$ variables 
will not increase the frequency with which they make correct predictions.

## Optimal Rules
Conditional of using a particular mental model, the most accurate prediction rule that the agent can use is one that 
predicts the state to be **B** whenever, given the observables under consideration, **B** is more likely. 
And it predicts **R** whenever **R** is more likely. Because we abstract away from the procedure through which the rule is updated, 
we will make predictions under the assumption that the agent has learned the parameters perfectly. That is, 
they have seen enough information that, under an updating procedure that converges to the truth---for example, 
correctly specified regression model or Bayes rule---they would have converged to the true parameters already. This provides an 
upper bound for how well any model can perform. 

We refer to a prediction rule as an *Optimal Rule* if it is the rule that a fully rational agent would 
converge to with perfect Bayesian learning. In this context, Bayesian learning corresponds to using Bayes rule to 
learn the coefficients $a = (a_1, a_2, ..., a_N)$ as well as the constant $K$. This means that in an $N$ dimensional 
space, the agent must learn $N+1$ parameters which corresponds to learning the halfspace in which the state 
is **B**. The optimal rule for the complex model coincides with the true data-generating process when learning 
is perfect and the parameters are identified.

An optimal rule conditional on model $M$ is the rule that, when restricted to the 
variables being considered, does the best at predicting the state. In this case, the agent is learning only 
the parameters that pertain the variables in $M$ plus a constant term. The following proposition illustrates exactly 
what the optimal prediction rules look like when the agent is able to learn perfectly. 

\begin{proposition}
Fix any model that considers $m\leq N$ variables and relabel the variable so that $x_1, ..., x_m$ are the variables 
that the model considers and $x_{m+1}, ..., x_N$ are the ones that the model does not consider. Relabel the 
coefficients $a = (a_1, ..., a_N)$ accordingly as well. 
\end{proposition}
*Proof*.
Let $M:=a_1x_1+...+a_mx_m$ and $M^c = a_{m+1}x_{m+1}+...+a_Nx_N+k$ and define the latent variable $y^L := M +  M^c$. 
The optimal prediction rule for model $M$ predicts the state to be $\textbf{B}$ whenever 
$P[y^L|M]\geq \frac{1}{2}$ and predicts $\textbf{R}$ otherwise.

Using the fact that for a random variable $z\sim N(\mu_z, \sigma_z)$ we have that $P[z\geq \mu_z]=\frac{1}{2}$ and 
noticing that $M^c$ and $y^L|M$ are Normally distributed with means $\mathbb{E}[M^c]$ and $M+\mathbb{E}[M^c]$ 
respectively, it is easy to see that $P[y^L\geq 0 | M] \geq P[y^L\geq M+\mathbb{E}[M^c | M]]=\frac{1}{2}$ whenever 
$M+\mathbb{E}[M^c]\geq 0$. Similarly, whenever $M+\mathbb{E}[M^c]< 0$ we will have that 
$P[y^L\geq 0 | M] < P[y^L\geq M+\mathbb{E}[M^c | M]]=\frac{1}{2}$. Therefore, the optimal rule is to predict 
$\mathbf{B}$ whenever $a_1x_1+...+a_kx_k\geq -\mathbb{E}[M^c]$ and $\textbf{R}$ otherwise.
$\quad \square$

 \

With limited data, it is not guaranteed that the agent is able to perfectly learn the parameters required for the optimal rule. 
However, the notion of optimal rules is useful for determining a benchmark for how well each model can perform, as well as 
the scenarios in which we can expect polarization to arise. 
The next section uses the notion of optimal rules to formalize the concept of polarization and to establish the 
main prediction of the theory. 

## Polarization
We take the definition of polarization from @Haghtalab2021: two agents are said to be 
polarized if, when observing the realization of X, they predict the state to be different. That is, 
they both observe the same draw of $(x_1, ..., x_N)$ but one agent predicts the state to be **B** and the other 
predicts the state to be **R**.

Within out framework---as in @Haghtalab2021---polarization may persist even when agents have 
access to unlimited data. There are two factors that dare necessary for polarization to arise: the use of different 
models and the realization of the observables must be such that the models will make different predictions. It is important
to observe that even when agents are using different models, not all realiations of $X$ will produce polarized 
predictions. The following 2-dimensional example illustrates why the use of different models is necessary but not sufficient 
for polarization to arise.

**Example.**
_The optimal rule for the complex model coincides with the truth, it predicts **B** if $x_2-1\geq x_1$. 
In addition, there are two simple models: one that considers only $x_1$---call it $M_1$---and one that considers 
only $x_2$---call it $M_2$. The optimal rules for $M_1$ and $M_2$ are threshold rules that predict the state is **B** 
if the value of $x_i$ is below (above) the threshold $t_i := \mu_{-i} + k$ and the predicted state is **R** otherwise.
These prediction rules are illustrated in . As Figure \ref{examp} makes clear, both rules agree on the predicted state for 
some values of $(x_1, x_2)$ and disagree for other values. The regions in which the rules make different predictions
are the disagreement zones._

```{r, out.width='65%', fig.align='center', fig.cap='\\label{examp} Polarization regions for two single variable models'}
knitr::include_graphics(here("diagram_polariazation_2variables.png"))
```

For any pair of agents who observe the same realization of the observables and use different models, it is possible
to determine whether they will be polarized or not by looking at the predictions made by the optimal rule corresponding to their 
models. Polarizing observations---those for which the predictions are different---are different for each pair of models. 
Also notice that we determine whether an observation is polarizing or not by using the optimal rules. We do not expect 
subjects in our experiment to converge to the optimal rule, this is a theoretical construct that allows
us to formalize Prediction 2.

**Prediction 2 :** Polarization will arise more often when the theory predicts so. That is, when two agents who have 
different models of the world, are facing a realization of observables that polarizing for their models.

# Experimental Design
```{r read-data-all}
data <- fread(here("data", "clean", "all.csv"))
part1 <- fread(here("data", "clean", "part1.csv"))
part2 <- fread(here("data", "clean", "part2.csv"))
count_models <- fread(here("data", "clean", "count_models.csv"))
pairs <- fread(here("data", "clean", "pairs.csv"))
```


Subjects for the experimet were recruited from NYU's CESS-lab subject pool. 
We recruited `r data[, uniqueN(participant.code)]` undergraduate students who participated in this experiment in person 
during the summer and fall of 2023. The experiment was coded using oTree (@otree) and it consisted of two parts:
Part 1 was meant to expose the subjects to the data generating process
and allow them to learn how to predict the state; part 2 was meant to elicit the models of the world that subjects might have
developed in part 1.

Part 1 consited of 20 rounds in which subjects observed the realizations of all of the observables and were asked to predict
the state. We had 5 observable variables called variable 1, 2, 3, 4 and 5. The variables were independent and Normally 
distributed with means and variances as described in Table \ref{params}. 

The state was determined by the following linear rule: $11x_1+6x_2+4.5x_3+2.4x_4+k \geq 0$. 
The values of the coefficients were chosen
so that all variables had a similar but unique level of informativeness when considered on their own. The constant $k$ was 
chosen so that the state was **B** in 50% of the cases. 
The last column of Table \ref{params} shows the probability of the optimal prediction rule making a correct prediction 
when for each single-variable model. This is the measure that we use to determine the informativeness of each variable.

| Variable | Mean | Variance | Modified Variable | Modified Variance | Informativeness |
| -------- | ---- | -------- | ----------------- | ----------------- | --------------- |
| $x_1$    | 0    |   1      | $11x_1$           |         11        |        .65      |
| $x_2$    | -10  |   2      | $6x_2$            |         12        |        .67      |
| $x_3$    | 5    |   3      | $4.5x_3$          |        13.5       |        .69      |
| $x_4$    | -5   |   4      | $2.4*x_4$         |         12        |        .63      |
| $x_5$    | 100  |   5      | $0x_5$            |         0         |         0       |

Table: Parameters for the Data Generating Process \label{params}

In each round, subjects were asked to predict the state. They were given feedback on whether their prediction was correct or not 
and they had access to the entire history of the game. 
Throughout the experiment, subjects were paired with another participant at random. Both subjects in a pair were shown exactly 
the same realization of the variables and had to predict the same state. This allows us to determine whether the pair was 
polarized for that particular observation. Subjects were not told that there was someone else observing the same realizations as them. 

Part 2 started immediatly after part 1 and had 70 rounds\footnote{In the first 2 sessions we had 40 rounds in part 2
and the experiment was done in under 30 minutes. Because the experiment concluded in less time than expected, we increased the 
number of rounds to 70 for the following sessions. Only 12 of our subjects participated in the sessions with 40 rounds, all others 
were in sessions with 70 rounds for part 2}. 
In instead of showing subjects the realizations of all 5 variables, they were 
told that they can disclose up to $m$ variables, where $m$ was drawn at random from the set $\{1, 2, ..., 5\}$. m was redrawn
independently across rounds and across subjects. 

We use the random assignment of $m$ to investigate whether the subjects are using 
siumple or complex models. If allowing subjects to disclose an additional variable improves their prediction accuracy, 
then it must be that they are in fact using the additional information for prediciton. If, on the other hand, they do not perform 
better when they have access to more information, it must be that, although they are disclosing an additional variable, 
they do not correctly incorporate the infomation into their prediction rule. Thus, if we observe that the performance of our 
subjects does not improve when they are allowed to disclose $m+1$ variables, 
relative to their performance when they were allowed to disclose only $m$ variables, their model of the world must be of size $m$
or lower. This feature design allows us to test predictions 1 and 1B and determine the number of variables that subjects are using.

In order to explore the polarization prediction, we use the fact that subjects are in fixed pairs throughout the experiment.
Both subjects in a pair observe the same realization of all the variables in each round. Therefore, we can determine whether
they are polarized or not by looking at whether they make the same prediction or not. We can then determine whether the theory 
predicts that they should be polarized or not by looking at what the optimal rules for those models dictate.
We use the optimal rules that correcpond to the variables that they chose to reveal in that round. It could be that
although they are revealing certain variables, they are not using them for prediction. Since we do not have a good method to 
determine which variables they actually pay attention to, we use the variables that they reveal as a proxy for the variables 
that they are using for prediction and apply the optimal rules for those models.


# Results

In this section we present the results from the experiment. We start by exploring the general learning patterns of the subjects
as well as the model choices. We then look at the effect of allowing subjects to disclose an additional variable and lastly we
look at the polarization results.

\subsection{Learning}
In the first part of the experiment, subjects were asked to predict the state given the realization of all the variables.
Having access to all the information they could predict the state perfectly if they managed to learn the parameters of the
hyperplane that determines the state. Figure \ref{learningAll} shows the share of correct predictions across rounds in part 1. 
We see that subjects are able to learn how to predict the state at a rate that is higher than random. They initially predict
the state corectly about 50% of the time, which is consistent with random guessing. The share of correct guesses increases to 
`r 100*part1[subsession.round_number>15, mean(player.correct)]`% by the end of part 1. And this last number is 
significantly higher than 50% 
(p-value = `r t.test(part1[subsession.round_number>10, player.correct], mu=0.5)$p.value`). 
They continue to learn even after the first 20 rounds when we look at the rounds in which subjects had access to all the information.
By the end of the experiment the rate of correct predictions is 
`r 100*part2[subsession.round_number>30, mean(player.correct)]`% which is still significantly higher than 50%
(p-value = `r t.test(part2[subsession.round_number>30, player.correct], mu=0.5)$p.value`).

```{r learningAll, out.width='65%', fig.align='center', fig.cap='\\label{learningAll} Share of correct predictions across rounds by performance in rounds 20 to 40'}
knitr::include_graphics(here("computed_objects", "figures", "learning_all.png"))
```

We also observe that there is heterogeneity in how our subjects learn. In particular, in the first 20 rounds of part 2 we are 
able to identify two distinct groups of subjects: those who guess more than 50% of the states correctly and those who guess
50% of fewer of them correctly. As can seen in \ref{p2groups}, these two groups have very different learning patterns 
throughout the experiment. The group that does better than random in rounds 20 to 40 (the first 20 rounds of part 2) consists 
of `r part2[better_random_2040==TRUE, uniqueN(participant.code)]` subjects, which account for 
`r 100*part2[better_random_2040==TRUE, uniqueN(participant.code)]/part2[, uniqueN(participant.code)]`% of the sample. These subjects 
improved their performance from 50% to `r 100*part2[better_random_2040==TRUE & round_number_modif<41, mean(player.correct)]`% in the 
first 20 rounds of part 2, and continued to improve to get to 
`r 100*part2[better_random_2040==TRUE & round_number_modif>50 & round_number_modif<61, mean(player.correct)]`% by the end of part 2. 
Meanwhile, the group that does worse than random in rounds 20 to 40 consists of `r part2[better_random_2040==FALSE, uniqueN(participant.code)]`
subjects and they do not seem to learn how to predict the state more accurately even after all 60 rounds of the experiment.
Whenever it is relevant, we will show results separately for these two groups.

```{r p2groups, out.width='65%', fig.align='center', fig.cap='\\label{p2groups} Share of correct predictions across rounds'}
knitr::include_graphics(here("computed_objects", "figures", "p2_correct_rounds.png"))
```

## Simplicity

In part 2, subjects were allowed to disclose up to $m$ variables before making their predictions. and $m$ was assigned at random 
for every subject in every round. We use this random assignment to understand if allowing them to reveal an additional variable 
improves their prediction accuracy. If it does, it must be that they are using the additional information for prediction. 
Figure \ref{simplicity} shows the share of correct predictions by the number of variables that subjects chose to disclose. 
Figure \ref{simplicity2} shows the number of variables that subjects chose to disclose by the number they we allowed to disclose.
On average, they always disclose fewer variables than what is available to them. This is a strong indication that at least some
of the subjects are using simple models. 
Further evidence is provided by the accuracy of their predictions as discussed in what follows

```{r simplicity2, out.width='65%', fig.align='center', fig.cap='\\label{simplicity2} Number of disclosed variables by number of allowed variables'}
knitr::include_graphics(here("computed_objects", "figures", "revealed_available.png"))
```

```{r simplicity, out.width='65%', fig.align='center', fig.cap='\\label{simplicity} Share of correct predictions by number of disclosed variables'}
knitr::include_graphics(here("computed_objects", "figures", "revealed_variables_pooled.png"))
```

We see that there is a sharp increase when going from 1 to 2 variables, but no significant increase when going from 2 to 3 and
beyond. This suggests that on average subjects are using mental models that consider 2 variables. However, since subjects were 
allowed to reveal fewer variables than what was assigned to them, the estimates in \ref{simplicity} might be biased. To account 
for this, we also look at the share of correct predictions by the number of variables that subjects were allowed to disclose. The results 
are presented in Table \ref{regression_table}. The table presents the regression estimates for the model with the assigned number of
variables as well as the model with the number of variables that subjects chose to disclose. The last column clusters standard errors
at the group level.


```{r regressionTable}
regs <- readRDS(here("computed_objects", "tables", "regressions.rds"))

regression_table <- huxreg("Disclosed" = regs$m2,
                           "Allowed" = regs$m1,
                           "Allowed" = regs$m4,
       coefs = c("Zero variables" = "C(revealed_variables_count)0",
                 "One variable" = "C(player.number_variables)1",
                 "One variable" = "C(revealed_variables_count)1",
                 "Two variables" = "C(player.number_variables)2",
                 "Two variables" = "C(revealed_variables_count)2",
                 "Three variables" = "C(player.number_variables)3",
                 "Three variables" = "C(revealed_variables_count)3",
                 "Four variables" = "C(player.number_variables)4",
                 "Four variables" = "C(revealed_variables_count)4",
                 "Five variables" = "C(player.number_variables)5",
                 "Five variables" = "C(revealed_variables_count)5"),
        statistics = c("N. obs." = "nobs", "R squared" = "r.squared")
        )

row <- data.frame("Clustered SE", "NO", "NO", "Group")
regression_table <- insert_row(regression_table, row, after = 13)
caption(regression_table) <- "Regression results, dependent variable is the share of correct predictions."
label(regression_table) <- "regression_table"
regression_table
```


```{r hypothesesAll}
# load the tables
tables <- readRDS(here("computed_objects", "tables", "hypothesis_tests.rds"))
results_tbl1 <- tables$tbl1
results_tbl2 <- tables$tbl2
results_tbl4 <- tables$tbl4
# assigned variables
test_assigned <- results_tbl1 %>% as_huxtable() %>% set_col_width(c(0.5, 0.25, 0.25))
label(test_assigned) <- "test_assigned"
caption(test_assigned) <- "Hypothesis tests for the regression with the number of variables that subjects were allowed to disclose."

# clustered standard errors
test_clustered <- results_tbl4 %>% as_huxtable() %>% set_col_width(c(0.5, 0.25, 0.25))
label(test_clustered) <- "test_clustered"
caption(test_clustered) <- "Hypothesis tests for the regression with the number of variables that subjects were allowed to disclose and clustered standard errors."

# revealed variables
test_revealed <- results_tbl2 %>% as_huxtable() %>% set_col_width(c(0.5, 0.25, 0.25))
label(test_revealed) <- "test_revealed"
caption(test_revealed) <- "Hypothesis tests for the regression with the number of variables that subjects chose to disclose."

```


We are interested in testing whether the probability of guessing the state correctly increases when subjects are allowed to 
disclose an additional variable. This corresponds to testing the linear hypotheses $\beta_{m+1}-\beta_m=0$ for $m=1, 2, 3, 4$.
We see that the probability of guessing the state when subjects
are allowed to disclose 2 variables is significantly higher than when they are allowed to disclose only 1 variable. And the effect 
of having access to the additional variable is `r results_tbl4[2, Estimate]` percentage points. Allowing 3, 4 and 5 variables has no 
effect, which sugests that subjects are using models that consider 2 variables. All the hypothesis tests are reported in 
Tables \ref{test_assigned},\ref{test_revealed} and \ref{test_clustered} in the appendix.



## Model Choices
Since we now know that subjects are using models that consider 2 variables, we can look at the specific models that they are using.
We look at two particular features: Are they choosing the most informative models? and do they improve their model choices over time?

Conditional on the number of variables that they were allowed to disclose, subjects explore an average of 
`r mean(count_models$number_models)` models thoughout the experiment\footnote{When there are 5 single-variable models, 10 models 
for 2 and 3 variables each, 5 models that consider 4 variables plus the complex and the empty models for an average of 22.2 
possible models to choose from in each round}. 
Figure \ref{modelCount} shows the distribution of the number of models
that subjects explored for each number of allowed variables. For the majority of subjects we do not see wide exploration across
models, which suggests some level of stickiness in their model choices. 

```{r modelCount, out.width='65%', fig.align='center', fig.cap='\\label{modelCount} Distribution of the number of models explored by number of allowed variables'}
knitr::include_graphics(here("computed_objects", "figures", "model_count.png"))
```

We also look at the informativeness of the models that subjects chose to explore. Figure \ref{modelInformativeness} shows how the 
average informativeness of the chose models changed across rounds. For all numbers of allowed variables, the average informativeness
stays relatively constant throughout the experiment. This suggests that subjects are not learning to choose more informative models, 
which is also consitent with the fact that the fact that our subjecst are not exploring a wide range of models.

```{r modelInformativeness, out.width='65%', fig.align='center', fig.cap='\\label{modelInformativeness} Average informativeness of the models chosen by number of allowed variables'}
knitr::include_graphics(here("computed_objects", "figures", "p1_performance_throughout.png"))
```

If we focus our attention at only the rounds in which they could disclose up to 2 variables, we see high heterogeneity in the model 
choices. This heterogeneity does not decrease over time. The bottom panel of Figure \ref{modelChoices} shows the distribution of 
the models that subjects chose in such rounds. The top pannel shows the comparison between the informativeness of the model and 
the share of correct guesses for subjects who chose that model.
Figure \ref{modelChoicesAll} in the appendix shows the distribution for all possible models.

```{r modelChoices, out.width='65%', fig.align='center', fig.cap='\\label{modelChoices} Distribution of the models chosen'}
knitr::include_graphics(here("computed_objects", "figures", "model_choices_2vars.png"))
```

Put together with the fact that subjects continue to learn and predict better throughout the experiment, this suggests that 
they become better at using the same mental models, and not that they are learning to use more informative models.

## Polarization Results
As for polarization, we see that it arises in `r pairs[predicted_polarization==TRUE & round_number_modif>40, mean(polarized)]` of the guesses in which 
it was predicted. While it arises only in `r pairs[predicted_polarization==FALSE & round_number_modif>40, mean(polarized)]` of the guesses in which it 
was not predicted\footnote{We use all the data after 20 rounds of part 2 to allow for some learning on how to select models}. 
This difference is statistically significant 
(p-value = `r t.test(pairs[predicted_polarization==TRUE & round_number_modif>40, polarized], pairs[predicted_polarization==FALSE & round_number_modif>40, polarized])$p.value`).

```{r polarization, out.width='65%', fig.align='center', fig.cap='\\label{polarization} Polarization by whether it was predicted or not'}
knitr::include_graphics(here("computed_objects", "figures", "polarization.png"))
```

The prediction is that 100% of the pairs would be polarized whenever the optimal rules for their models predict polarization and 
none of them would be when it is not predicted. There are two factors that may influence the difference with our results. The first 
is that subjects might not have converged to the optimal rules for the models that they selected. This would induce discrepancy with 
the prediciton. The other is that we use take the varaibles that they chose to disclose in that round as the model that they are 
using. However, from the discussion in the previous section we know that even though subjects are disclosing several variables, 
they do not necessarily use the extra information. This would induce discrepancy with the prediction as well. Nonetheless, we 
take the fact that polarization is higher when predicted by the theory as an indication that subjects are using simple models to 
some extent. Figure \ref{polarizationRounds} shows the share of polarized pairs by whether polarization was predicted or not 
across rounds.

```{r polarizationRounds, out.width='65%', fig.align='center', fig.cap='\\label{polarizationRounds} Polarization by whether it was predicted or not'}
knitr::include_graphics(here("computed_objects", "figures", "polarization_rounds_predicted.png"))
```

# Conclusion


\newpage
\appendix

# Appendix
```{r TestTables}
test_revealed
test_assigned
test_clustered
```

```{r modelChoicesAll, out.width='65%', fig.align='center', fig.cap='\\label{modelChoicesAll} Distribution of the models chosen'}
knitr::include_graphics(here("computed_objects", "figures", "model_choices_all.png"))
```


