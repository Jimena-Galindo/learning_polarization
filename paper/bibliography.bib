@article{otree,
   abstract = {oTree is an open-source and online software for implementing interactive experiments in the laboratory, online, the field or combinations thereof. oTree does not require installation of software on subjects' devices; it can run on any device that has a web browser, be that a desktop computer, a tablet or a smartphone. Deployment can be internet-based without a shared local network, or local-network-based even without internet access. For coding, Python is used, a popular, open-source programming language. www.oTree.org provides the source code, a library of standard game templates and demo games which can be played by anyone.},
   author = {Daniel L. Chen and Martin Schonger and Chris Wickens},
   doi = {10.1016/J.JBEF.2015.12.001},
   issn = {2214-6350},
   journal = {Journal of Behavioral and Experimental Finance},
   keywords = {Classroom experiments,Experimental economics,Field experiments,Laboratory experiments,Online experiments,Software},
   month = {3},
   pages = {88-97},
   publisher = {Elsevier},
   title = {oTree—An open-source platform for laboratory, online, and field experiments},
   volume = {9},
   year = {2016},
}
@article{Haghtalab2021,
   author = {Nika Haghtalab and Matthew O. Jackson and Ariel D. Procaccia},
   issue = {19},
   journal = {PNAS},
   title = {Belief polarization in a complex world: A learning theory perspective},
   volume = {118},
   year = {2021},
}
@generic{Ortoleva2012,
   abstract = {Bayes' rule has two well-known limitations: 1) it does not model the reaction to zero-probability events; 2) a sizable empirical evidence documents systematic violations of it. We characterize axiomatically an alternative updating rule, the Hypothesis Testing model. According to it, the agent follows Bayes' rule if she receives information to which she assigned a probability above a threshold. Otherwise, she looks at a prior over priors, updates it using Bayes' rule for secondorder priors, and chooses the prior to which the updated prior over priors assigns the highest likelihood. We also present an application to equilibrium refinement in game theory.},
   author = {Pietro Ortoleva},
   doi = {10.1257/aer.102.6.2410},
   issn = {00028282},
   issue = {6},
   journal = {American Economic Review},
   month = {10},
   note = {Prior over all possible priors. If an event to which I assigned probability zero happens, I use the prior over priors to choose a new prior. Otherwise I stick with the prior I already had and update bayesianly there.},
   pages = {2410-2436},
   title = {Modeling the change of paradigm: Non-bayesian reactions to unexpected news},
   volume = {102},
   year = {2012},
}
@article{Augenblick2021,
   abstract = {When a Bayesian learns new information and changes her beliefs, she must on average become concomitantly more certain about the state of the world. Consequently, it is rare for a Bayesian to frequently shift beliefs substantially while remaining relatively uncertain, or, conversely, become very confident with relatively little belief movement. We formalize this intuition by developing specific measures of movement and uncertainty reduction given a Bayesian's changing beliefs over time, showing that these measures are equal in expectation and creating consequent statistical tests for Bayesianess. We then show connections between these two core concepts and four common psychological biases, suggesting that the test might be particularly good at detecting these biases. We provide support for this conclusion by simulating the performance of our test and other martingale tests. Finally, we apply our test to data sets of individual, algorithmic, and market beliefs.},
   author = {Ned Augenblick and Matthew Rabin},
   doi = {10.1093/qje/qjaa043},
   issn = {15314650},
   issue = {2},
   journal = {Quarterly Journal of Economics},
   month = {5},
   note = {Bayesian updating is characterized by reduction in uncertainty after the update},
   pages = {933-985},
   publisher = {Oxford University Press},
   title = {Belief movement, uncertainty reduction, and rational updating},
   volume = {136},
   year = {2021},
}
@article{Mailath2020,
   abstract = {People reason about uncertainty with deliberately incomplete models. How do people hampered by different, incomplete views of the world learn from each other? We introduce a model of “model-based inference.” Model-based reasoners partition an otherwise hopelessly complex state space into a manageable model. Unless the differences in agents’models are trivial, interactions will often not lead agents to have common beliefs or beliefs near the correct-model belief. If the agents’models have enough in common, then interacting will lead agents to similar beliefs, even if their models also exhibit some bizarre idiosyncrasies and their information is widely dispersed.},
   author = {George J. Mailath and Larry Samuelson},
   doi = {10.1257/aer.20190080},
   issn = {19447981},
   issue = {5},
   journal = {American Economic Review},
   month = {5},
   note = {model-based inference with incomplete models. Partition the state space into a more manageable model. Unless the differences in agents’ models are trivial, interactions will<br/>often not lead agents to have common beliefs or beliefs near the<br/>correct-model belief.<br/>THEY RESPOND TO THE BELIEFS OF OTHERS},
   pages = {1461-1501},
   publisher = {American Economic Association},
   title = {Learning under diverse world views: Model-based inference†},
   volume = {110},
   year = {2020},
}
@article{Levy2022,
   abstract = {We develop a dynamic model of political competition between two groups that differ in their subjective model of the data generating process for a common outcome. One group has a simpler model than the other group as they ignore some relevant policy variables. We show that policy cycles must arise and that simple world views—which can be interpreted as populist world views—imply extreme policy choices. Periods in which those with a more complex model govern increase the specification error of the simpler world view, leading the latter to overestimate the positive impact of a few extreme policy actions.},
   author = {Gilat Levy and Ronny Razin and Alwyn Young},
   doi = {10.1257/AER.20210154},
   issn = {19447981},
   issue = {3},
   journal = {American Economic Review},
   month = {3},
   pages = {928-962},
   publisher = {American Economic Association},
   title = {Misspecified Politics and the Recurrence of Populism},
   volume = {112},
   year = {2022},
}
@article{Esponda2021,
   abstract = {We consider an agent who represents uncertainty about the environment via a possibly misspecified model. Each period, the agent takes an action, observes a consequence, and uses Bayes' rule to update her belief about the environment. This framework has become increasingly popular in economics to study behavior driven by incorrect or biased beliefs. By first showing that the key element to predict the agent's behavior is the frequency of her past actions, we are able to characterize asymptotic behavior in general settings in terms of the solutions of a differential inclusion that describes the evolution of the frequency of actions. We then present a series of implications that can be readily applied to economic applications, thus providing off-the-shelf tools that can be used to characterize behavior under misspecified learning.},
   author = {Ignacio Esponda and Demian Pouzo and Yuichi Yamamoto},
   doi = {10.1016/j.jet.2021.105260},
   issn = {10957235},
   journal = {Journal of Economic Theory},
   keywords = {Asymptotic behavior,Bayesian learning,Berk-Nash equilibrium,Differential inclusion,Misspecified models},
   month = {7},
   note = {the agent takes an action, sees a consequence and updates their belief. Implications that characterize behavior:<br/>cyclic behavior,  misdirected learning as in HKS's overconfidence model.<br/><br/>REFERENCES OF MISSPECIFIED LEARNING},
   publisher = {Academic Press Inc.},
   title = {Asymptotic behavior of Bayesian learners with misspecified models},
   volume = {195},
   year = {2021},
}
@report{,
   abstract = {We develop a framework for assessing when a person will notice that a theory she has about the world is wrong, premised on the idea that people neglect information that they view (through the lens of their misconceptions) to be irrelevant. Focusing on the question of when a mistaken theory can persist in the long run even when attention is very cheap, we study the attentional stability of both general psychological biases-such as naivete about present bias or neglecting the redundancy in social information-and context-specific empirical misconceptions-such as false beliefs about medicinal side effects or financial investments. People discover their errors only when the data they deem relevant causes them to incidentally notice that their theory is wrong. We explore which combinations of errors and environments allow an error to persist. People tend to notice costly errors in a particular context when they are right about which factors matter even when they are wrong about how these factors matter, whereas errors that lead people to ubiquitously treat consequential factors as irrelevant will be stable across a broad class of environments. Environments designed to make such factors artificially germane can induce recognition of an error.},
   author = {Tristan Gagnon-Bartsch and Harvard Matthew and Rabin Harvard and Joshua Schwartzstein and Nava Ashraf and Francesca Bastianello and Max Bazerman and Katherine Coffman and Christine Exley and Erik Eyster and Drew Fudenberg and Nicola Gennaioli and Brian Hall and David Hirshleifer and Botond K˝ Oszegi and Spencer Kwon and George Loewenstein and Michael Luca and Kathleen Mcginn and Sendhil Mullainathan and Andrei Shleifer and Dmitry Taubinsky and Yale Gagnon-Bartsch and Rabin Thank},
   title = {Channeled Attention and Stable Errors},
   year = {2021},
}
@report{,
   abstract = {An important issue in decision making concerns the manner in which people process new information and update prior beliefs. The Bayesian updating rule, in combination with expected utility theory, is ubiquitous in economic theory, and its application is an important paradigm for examining decision making under risk. A number of experimental studies suggest, however, that people may often ignore prior information when forming beliefs, contrary to Bayes's rule. 1 Another heuristic for processing new information involves some form of reinforcement , where one is more likely to pick choices (actions) associated with successful past outcomes than choices associated with less successful outcomes. 2 These separate approaches often prescribe a similar course of action in the face of new information; however, this is not always the case. We construct an individual choice task in which Bayesian updating with expected utility maximization (BEU) sometimes coincides and sometimes opposes a "win-stay lose-shift" heu-ristic. Here, Bayesian updating after a successful outcome should lead a decision maker to make a change, while no change should be made after observing an unsuccessful outcome. We observe how one's propensity to use Bayes's rule is affected by whether this rule is aligned with the win-stay lose-shift heuristic or clashes with it. We also consider whether this propensity differs according to whether the information provided by an earlier outcome is accompanied by payment for the outcome and the corresponding feelings of success or failure. To the best of our knowledge, this is the first study to examine explicitly what happens when these forces work against one another. 3 Our constructed case where the reinforcement heu-ristic leads one astray can be applied more generally to situations where favorable direct). We thank Mark Brinkman and Arun Qamra for their help with designing the software and conducting the experimental sessions. We thank Doug Bernheim and an anonymous referee for valuable editorial comments. We have benefited from discussions with, and suggestions from, also provide strong evidence that experimental subjects are often not even close to being "perfect Bayesians." 2 The basic reinforcement learning models (e.g., Alvin E. Roth and Ido Erev, 1995; Erev and Roth, 1998) assume an initial propensity for a particular choice and utilize a payoff sensitivity parameter. Colin Camerer and Teck-Hua Ho (1998, 1999) combine reinforcement and belief learning by using experience-weights and updated levels of attraction. Case-based decision theory (Gilboa and Schmeidler, 1995, 2001) formalizes the thrust of reinforcement heuristics in a nonexpected utility framework wherein people follow a decision rule that chooses an act with the highest relative score, based on performance in past cases and the similarity of those cases to the current decision case. 3 Note that our setup is quite different from the "two-armed bandit" problem, where the separate machines have independent distributions rather than a common state. 1300},
   author = {By Gary Charness and Dan Levin and George Akerlof and Ted Bergstrom and Antonio Cabrales and James Choi and Carl Christ and Stefano DellaVigna and Matthew Ellman and Ido Erev and Guillaume Fréchette and Rod Gar-ratt and Itzhak Gilboa and Robin Hogarth and Edi Karni and Botond Koz-segi and David Laibson and Cade Massey and Jim Peck and Matthew Rabin and David Schmeidler and Bill Zame and Richard Zeckhauser},
   publisher = {Kahneman and Tversky},
   title = {When Optimal Choices Feel Wrong: A Laboratory Study of Bayesian Updating, Complexity, and Affect},
   url = {www.econ.ucsb.edu/gcsurvey/Bayesian_updating/.},
   year = {1971},
}
@article{Liang2020,
   abstract = {We develop a model of social learning from complementary information: short-lived agents sequentially choose from a large set of flexibly correlated information sources for prediction of an unknown state, and information is passed down across periods. Will the community collectively acquire the best kinds of information? Long-run outcomes fall into one of two cases: (i) efficient information aggregation, where the community eventually learns as fast as possible; (ii) "learning traps," where the community gets stuck observing suboptimal sources and information aggregation is inefficient. Our main results identify a simple property of the underlying informational complementarities that determines which occurs. In both regimes, we characterize which sources are observed in the long run and how often.},
   author = {Annie Liang and Xiaosheng Mu},
   doi = {10.1093/qje/qjz033},
   issn = {15314650},
   issue = {1},
   journal = {Quarterly Journal of Economics},
   month = {2},
   pages = {389-448},
   publisher = {Oxford University Press},
   title = {Complementary Information and Learning Traps},
   volume = {135},
   year = {2020},
}
@report{Epstein2008,
   abstract = {This paper models an agent in a multi-period setting who does not update according to Bayes' Rule, and who is self-aware and anticipates her updating behavior when formulating plans. Choice-theoretic axiomatic foundations are provided to capture updating biases that reflect excessive weight given to either prior beliefs, or, alternatively, to observed data. A counterpart of the exchangeable Bayesian learning model is also described. K. Non-Bayesian updating, temptation and self-control, overreaction, underreaction, learning, law of small numbers. JEL . D81. 1. I Epstein (2006) models an agent who does not update according to Bayes' Rule, but is self-aware and anticipates her updating behavior when formulating plans. He provides axiomatic foundations for his model in the form of a representation theorem for suitably defined preferences such that both the prior and the way in which it is updated are subjective. The model is nested in a three-period framework, where the agent updates once and consumption occurs only at the terminal time. This paper extends the model to an infinite horizon setting, thereby enabling it to address dynamic issues and making it more amenable to applications. The benchmark for the present model is the standard specification of utility in dynamic modeling, whereby utility at time t is given by U t (c) = E t ∞ τ=t δ τ−t u (c τ) t = 0, 1,. .. ,},
   author = {Larry G Epstein and Alvaro Sandroni and Massimo Marinacci and Joseph Perktold and Werner Ploberger and especially Igor Kopylov},
   journal = {Theoretical Economics},
   pages = {193-229},
   title = {Non-Bayesian updating: a theoretical framework We have benefitted from comments from two referees and an editor, and from conversations with Mark},
   volume = {3},
   url = {http://econtheory.org.},
   year = {2008},
}
@article{Schwartzstein2014,
   abstract = {What do we notice and how does this affect what we learn and come to believe? I present a model of an agent who learns to make forecasts on the basis of readily available information, but is selective as to which information he attends to: he chooses whether to attend as a function of current beliefs about whether such information is predictive. If the agent does not attend to some piece of information, it cannot be recalled at a later date. He uses Bayes' rule to update his beliefs given attended-to information, but does not attempt to fill in missing information. The model demonstrates how selective attention may lead the agent to persistently fail to recognize important empirical regularities, make systematically biased forecasts, and hold incorrect beliefs about the statistical relationship between variables. In addition, it identifies factors that make such errors more likely or persistent. The model is applied to shed light on stereotyping and discrimination, persistent learning failures and disagreement, and the process of discovery.},
   author = {Joshua Schwartzstein},
   doi = {10.1111/jeea.12104},
   issn = {15424774},
   issue = {6},
   journal = {Journal of the European Economic Association},
   month = {12},
   pages = {1423-1452},
   publisher = {Wiley-Blackwell},
   title = {Selective attention and learning},
   volume = {12},
   year = {2014},
}
@report{Acemoglu2006,
   abstract = {Most economic analyses presume that there are limited differences in the beliefs ("priors") of individuals, an assumption most often justified by the argument that sufficient common experiences and observations will eliminate disagreements. We investigate this claim using a simple model of learning. Two individuals with different priors observe the same infinite sequence of signals about some underlying parameter. Existing results in the literature establish that when individuals are certain about the interpretation of signals, under very mild conditions their assessments will eventually agree. In contrast, we look at an environment in which individuals are uncertain about the interpretation of signals, meaning that they also have non-degenerate probability distributions over the likelihood of signals given the underlying parameter. Assuming that the priors (about the parameter and the conditional distribution of the signals) have full support, we prove the following results. (1) Individuals will never agree, even after observing the same infinite sequence of signals. (2) Moreover, before observing the signals, they believe with probability 1 that their posteriors about the underlying parameter will fail to converge. (3) Observing the same sequence of signals may lead to a divergence of opinion rather than the typically-presumed convergence. (4) Asymptotic disagreement (and lack of learning) may prevail even under approximate certainty-i.e., as we look at the limit where uncertainty about the interpretation of signals disappears. In particular, when the family of probability distributions of signals given the parameter have "regularly-varying tails" (such as the Pareto, the log-normal, and the t-distributions), approximate certainty is not sufficient to restore asymptotic learning and asymptotic agreement between agents with different priors. Lack of common beliefs and common priors has important implications for economic behavior in a range of circumstances. We illustrate how the type of learning outlined in this paper interacts with economic behavior in various different situations, including games of common interest, coordination, asset trading and bargaining.},
   author = {Daron Acemoglu and Victor Chernozhukov and Muhamet Yildiz},
   keywords = {Bayesian learning,C72,D83 1,asymptotic disagreement,merging of opinions JEL Classification: C11},
   title = {Learning and Disagreement in an Uncertain World},
   year = {2006},
}
@article{,
   abstract = {Different agents need to make a prediction. They observe identical data, but have different models: they predict using different explanatory variables. We study which agent believes they have the best predictive ability—as measured by the smallest subjective posterior mean squared prediction error—and show how it depends on the sample size. With small samples, we present results suggesting it is an agent using a low-dimensional model. With large samples, it is generally an agent with a high-dimensional model, possibly including irrelevant variables, but never excluding relevant ones. We apply our results to characterize the winning model in an auction of productive assets, to argue that entrepreneurs and investors with simple models will be overrepresented in new sectors, and to understand the proliferation of “factors” that explain the cross-sectional variation of expected stock returns in the asset-pricing literature.},
   author = {José Luis Montiel Olea and Pietro Ortoleva and Mallesh M Pai and Andrea Prat},
   doi = {10.1093/qje/qjac015},
   issn = {0033-5533},
   issue = {4},
   journal = {The Quarterly Journal of Economics},
   month = {9},
   pages = {2419-2457},
   publisher = {Oxford University Press (OUP)},
   title = {Competing Models},
   volume = {137},
   year = {2022},
}
@article{Heidhues2021,
   abstract = {We establish convergence of beliefs and actions in a class of one‐dimensional learning settings in which the agent's model is misspecified, she chooses actions endogenously, and the actions affect how she misinterprets information. Our stochastic‐approximation‐based methods rely on two crucial features: that the state and action spaces are continuous, and that the agent's posterior admits a one‐dimensional summary statistic. Through a basic model with a normal–normal updating structure and a generalization in which the agent's misinterpretation of information can depend on her current beliefs in a flexible way, we show that these features are compatible with a number of specifications of how exactly the agent updates. Applications of our framework include learning by a person who has an incorrect model of a technology she uses or is overconfident about herself, learning by a representative agent who may misunderstand macroeconomic outcomes, and learning by a firm that has an incorrect parametric model of demand.},
   author = {Paul Heidhues and Botond Koszegi and Philipp Strack},
   doi = {10.3982/te3558},
   issn = {1933-6837},
   issue = {1},
   journal = {Theoretical Economics},
   pages = {73-99},
   publisher = {The Econometric Society},
   title = {Convergence in models of misspecified learning},
   volume = {16},
   year = {2021},
}
